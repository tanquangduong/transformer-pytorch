{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84d4068-4a23-46e4-a36b-b8704c5468db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformer.layer import MultiHeadAttention\n",
    "from transformer.mask import create_decoder_mask\n",
    "# Suppose we have a batch of 1 sequences (mini-batch size of 1)\n",
    "# Each sequence has 4 words (sequence length of 4)\n",
    "# We use 1 attention heads (h = 1) and the dimension of key/query (d_k) is 64\n",
    "batch_size = 1\n",
    "h = 1\n",
    "seq_len = 4\n",
    "d_k = 64\n",
    "pad_token_id = torch.tensor([0]) # padding token ID\n",
    "\n",
    "torch.manual_seed(68) # for reproducible result of random process\n",
    "input_tensor = torch.rand(batch_size, h, seq_len, d_k)\n",
    "query_k = input_tensor.clone()\n",
    "key_k = input_tensor.clone()\n",
    "value_k = input_tensor.clone()\n",
    "\n",
    "# Create decoder mask\n",
    "decoder_input_ids = torch.tensor([ 2, 68, 0, 0])\n",
    "pad_token_id = torch.tensor([0]) # padding token ID\n",
    "decoder_mask = create_decoder_mask(decoder_input_ids, pad_token_id, seq_len)\n",
    "\n",
    "# Call the attention function\n",
    "output, attention_scores = MultiHeadAttention.attention(query_k, key_k, value_k, d_k, mask=decoder_mask, dropout=nn.Dropout(0.1))\n",
    "\n",
    "print(f\"Decoder Mask:\\n{decoder_mask}\\n\")\n",
    "print(f\"Output Shape: {output.shape}\")\n",
    "print(f\"Attention Scores Shape: {attention_scores.shape}\")\n",
    "print(f\"Attention Scores:\\n{attention_scores}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
